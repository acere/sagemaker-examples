{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Face Detection and Recognition Model - SageMaker Inference Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "**Face detection**  \n",
    "In order to input only face pixels into the network, all input images are passed through a pretrained face detection and alignment model, [MTCNN detector](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html). The output of this model are landmark points and a bounding box corresponding to the face in the image. Using this output, the image is processed using affine transforms to generate the aligned face images which are input to the network.\n",
    "\n",
    "**Face feature generation**  \n",
    "For each face image, the model produces a fixed length embedding vector corresponding to the face in the image. The vectors from face images of a single person have a higher similarity than that from different persons. Therefore, the model is primarily used for face recognition/verification. It can also be used in other applications like facial feature based clustering.  \n",
    "\n",
    "**Model artifacts**     \n",
    "In this implementation, we use LResNet100E-IR, ResNet100 backend with [ArcFace](https://arxiv.org/abs/1801.07698) loss.   \n",
    "For both the MTCNN detector and the ResNet we use and use the pre-trained models from [ONNX Model Zoo](https://github.com/onnx/models), then import the [ONNX](http://onnx.ai/) files into MXNet model.\n",
    "\n",
    "**Deployment**   \n",
    "We deploy the pre-build models (detection+recognition) on SageMaker Managed real-time Endpoint.\n",
    "The steps are:\n",
    "\n",
    "1. Create an inference Python script with functions:\n",
    "    - `model_fn` loads your model\n",
    "    - `transform_fn` to handle inference requests.\n",
    "2. Package inference script, model artifacts, and additional files into a tarfile\n",
    "3. Upload the tarfile to an S3 bucket\n",
    "4. Create a `MXNetModel` ([documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/sagemaker.mxnet.html#sagemaker.mxnet.model.MXNetModel)), indicating framework version\n",
    "5. Deploy a predictor, indicating the number of instances and the instance type\n",
    "\n",
    "![Deploy Diagram](./images/sm_deploy_MXNet.png)\n",
    "\n",
    "Here's the structure we will compress and upload to S3, and that will be replicated in the Endpoint instance.\n",
    "```\n",
    "Model\n",
    "|-- code\n",
    "|   |-- helper.py\n",
    "|   |-- inference.py\n",
    "|   |-- mtcnn_detector.py\n",
    "|   `-- requirements.txt\n",
    "|-- mtcnn-model\n",
    "|   |-- det1-0001.params\n",
    "|   |-- det1-symbol.json\n",
    "|   |-- det1.caffemodel\n",
    "|   |-- det1.prototxt\n",
    "|   |-- det2-0001.params\n",
    "|   |-- det2-symbol.json\n",
    "|   |-- det2.caffemodel\n",
    "|   |-- det2.prototxt\n",
    "|   |-- det3-0001.params\n",
    "|   |-- det3-symbol.json\n",
    "|   |-- det3.caffemodel\n",
    "|   |-- det3.prototxt\n",
    "|   |-- det4-0001.params\n",
    "|   |-- det4-symbol.json\n",
    "|   |-- det4.caffemodel\n",
    "|   `-- det4.prototxt\n",
    "`-- resnet100.onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional references:\n",
    "\n",
    "- https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/mxnet_onnx_superresolution\n",
    "- https://sagemaker.readthedocs.io/en/latest/using_mxnet.html#serve-an-mxnet-model\n",
    "- https://github.com/onnx/models/tree/master/vision/body_analysis/arcface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and environment setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing the necessary libraries to run this notebook.  \n",
    "Differently from the `local` version of this notebook, we don't need to install addition libraries in the notebook kernel. The `requirement.txt` file will be included in the tarbool, and the additional libraries will be installed in the Framework image at deployment time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import sagemaker as sm\n",
    "from sagemaker.mxnet import MXNetModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define AWS environment and SageMaker objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = sm.Session()\n",
    "sm_client = sm_session.sagemaker_client\n",
    "region = sm_session.boto_region_name\n",
    "role = sm.get_execution_role()\n",
    "bucket = sm_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a prefix for all the files and artifacts of this demo, to easily identify the relevant object after uploading to S3.  \n",
    "We also define few variables to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"facedetection-mxnet\"\n",
    "\n",
    "framework = \"mxnet\"\n",
    "framework_version = \"1.8.0\"\n",
    "cpu_instance_type = \"ml.m5.xlarge\"\n",
    "gpu_instance_type = \"ml.g4dn.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-built Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = Path(\"model\")\n",
    "code_local_path = model_local_path / \"code\"\n",
    "images_local_path = Path(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_mtcnn_model(i, dirname: str):\n",
    "    base_url = f\"https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{i+1}\"\n",
    "    mx.test_utils.download(url=f\"{base_url}-0001.params\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}-symbol.json\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}.caffemodel\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}.prototxt\", dirname=dirname)\n",
    "    return \"Done\"\n",
    "\n",
    "\n",
    "mtcnn_local_path = model_local_path / \"mtcnn-model\"\n",
    "\n",
    "[download_mtcnn_model(i, dirname=mtcnn_local_path) for i in range(4)]\n",
    "\n",
    "print(f\"MTCNN artifacts downloaded to `{mtcnn_local_path}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download onnx model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcface_local_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/resnet100.onnx\",\n",
    "    dirname=model_local_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference script\n",
    "The inference script is in `model/code`. In the same folder, there are also the support libraries and `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize {code_local_path}/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that the inference script works as expected. to test it, we need to download the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download first image\n",
    "image1_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/player1.jpg\",\n",
    "    dirname=images_local_path,\n",
    ")\n",
    "# Download second image\n",
    "image2_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/player2.jpg\",\n",
    "    dirname=images_local_path,\n",
    ")\n",
    "\n",
    "img1 = cv2.imread(image1_path)\n",
    "img2 = cv2.imread(image2_path)\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Image1\")\n",
    "ax[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "ax[1].set_title(\"Image2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {code_local_path}/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compressed archive\n",
    "We can now compress the folder containing the scripts and the pre-built model and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compressed = shutil.make_archive(\n",
    "    \"model\", format=\"gztar\", root_dir=model_local_path\n",
    ")\n",
    "model_uri = sm_session.upload_data(\n",
    "    path=model_compressed, key_prefix=f\"{prefix}/{model_local_path.name}\"\n",
    ")\n",
    "print(f\"Compressed Model (scripts and model weights) uploaded to:\\n{model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sm = MXNetModel(\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=framework_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model_sm.deploy(initial_instance_count=1, instance_type=gpu_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n",
    "We can finally test the model. We will generate features for two test images and then compute two distance metrics between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = predictor.predict(img1)\n",
    "img1_preprocessed, out1 = out[\"preprocessed_image\"], np.array(\n",
    "    out[\"feature_vector\"], dtype=float\n",
    ")\n",
    "out = predictor.predict(img2)\n",
    "img2_preprocessed, out2 = out[\"preprocessed_image\"], np.array(\n",
    "    out[\"feature_vector\"], dtype=float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(np.transpose(img1_preprocessed, (1, 2, 0)))\n",
    "ax[0].set_title(\"Image1_preprocessed\")\n",
    "ax[1].imshow(np.transpose(img2_preprocessed, (1, 2, 0)))\n",
    "ax[1].set_title(\"Image2_preprocessed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute squared distance between embeddings\n",
    "dist = np.sum(np.square(out1 - out2))\n",
    "\n",
    "# Compute cosine similarity between embedddings\n",
    "sim = np.dot(out1, out2.T)\n",
    "\n",
    "# Print predictions\n",
    "print(f\"Distance = {dist:.4f}\")\n",
    "print(f\"Similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the average inference time. This is not a rigoruous benchmark, but it gives us an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "predictor.predict(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup of resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "mxnet18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
