{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Predicting Customer Churn\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "In this demo, you are going to learn how to use SageMaker DataWrangler to prepare your data the data to train a classification model to predict if a customer is likely to churn out of a music streaming service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### What is Customer Churn and why is it important for businesses?\n",
    "Customer churn, or customer retention/attrition, means a customer has the tendency to leave and stop paying for a business. It is one of the primary metrics companies want to track to get a sense of their customer satisfaction, especially for a subscription-based business model. The company can track churn rate (defined as the percentage of customers churned during a period) as a health indicator for the business, but we would love to identify the at-risk customers before they churn and offer appropriate treatment to keep them with the business, and this is where machine learning comes into play.\n",
    "\n",
    "### Use Cases for Customer Churn\n",
    "\n",
    "Any subscription-based business would track customer churn as one of the most critical Key Performance Indicators (KPIs). Such companies and industries include Telecom companies (cable, cell phone, internet, etc.), digital subscriptions of media (news, forums, blogposts platforms, etc.), music and video streaming services, and other Software as a Service (SaaS) providers (e-commerce, CRM, Mar-Tech, cloud computing, video conference provider, and visualization and data science tools, etc.)\n",
    "\n",
    "### Define Business problem\n",
    "\n",
    "To start with, here are some common business problems to consider depending on your specific use cases and your focus:\n",
    "\n",
    " * Will this customer churn (cancel the plan, cancel the subscription)?\n",
    " * Will this customer downgrade a pricing plan?\n",
    " * For a subscription business model, will a customer renew his/her subscription?\n",
    "\n",
    "### Machine learning problem formulation\n",
    "\n",
    "#### Classification: will this customer churn?\n",
    "\n",
    "To goal of classification is to identify the at-risk customers and sometimes their unusual behavior, such as: will this customer churn or downgrade their plan? Is there any unusual behavior for a customer? The latter question can be formulated as an anomaly detection problem.\n",
    "\n",
    "#### Time Series: will this customer churn in the next X months? When will this customer churn?\n",
    "\n",
    "You can further explore your users by formulating the problem as a time series one and detect when will the customer churn.\n",
    "\n",
    "### Data Requirements\n",
    "\n",
    "#### Data collection Sources\n",
    "\n",
    "Some most common data sources used to construct a data set for churn analysis are:\n",
    "\n",
    "* Customer Relationship Management platform (CRM), \n",
    "* engagement and usage data (analytics services), \n",
    "* passive feedback (ratings based on your request), and active feedback (customer support request, feedback on social media and review platforms).\n",
    "\n",
    "#### Construct a Data Set for Churn Analysis\n",
    "\n",
    "Most raw data collected from the sources mentioned above are huge and often needs a lot of cleaning and pre-processing. For example, usage data is usually event-based log data and can be more than a few gigabytes every day; you can aggregate the data to user-level daily for further analysis. Feedback and review data are mostly text data, so you would need to clean and pre-process the natural language data to be normalized, machine-readable data. If you are joining multiple data sources (especially from different platforms) together, you would want to make sure all data points are consistent, and the user identity can be matched across different platforms.\n",
    "           \n",
    "#### Challenges with Customer Churn\n",
    "\n",
    "* Business related\n",
    "    * Importance of domain knowledge: this is critical when you start building features for the machine learning model. It is important to understand the business enough to decide which features would trigger retention.\n",
    "* Data issues\n",
    "    * fewer churn data available (imbalanced classes): data for churn analysis is often very imbalanced as most of the customers of a business are happy customers (usually).\n",
    "    * User identity mapping problem: if you are joining data from different platforms (CRM, email, feedback, mobile app, and website usage data), you would want to make sure user A is recognized as the same user across multiple platforms. There are third-party solutions that help you tackle this problem.\n",
    "    * Not collecting the right data for the use case or Lacking enough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection\n",
    "\n",
    "You will use generated music streaming data that is simulated to imitate music streaming user behaviors. The data simulated contains 1100 users and their user behavior for one year (2019/10/28 - 2020/10/28). Data is simulated using the [EventSim](https://github.com/Interana/eventsim) and does not contain any real user data.\n",
    "\n",
    "* Observation window: you will use 1 year of data to generate predictions.\n",
    "* Explanation of fields:\n",
    "    * `ts`: event UNIX timestamp\n",
    "    * `userId`: a randomly assigned unique user id\n",
    "    * `sessionId`: a randomly assigned session id unique to each user\n",
    "    * `page`: event taken by the user, e.g. \"next song\", \"upgrade\", \"cancel\"\n",
    "    * `auth`: whether the user is a logged-in user\n",
    "    * `method`: request method, GET or PUT\n",
    "    * `status`: request status\n",
    "    * `level`: if the user is a free or paid user\n",
    "    * `itemInSession`: event happened in the session\n",
    "    * `location`: location of the user's IP address\n",
    "    * `userAgent`: agent of the user's device\n",
    "    * `lastName`: user's last name\n",
    "    * `firstName`: user's first name\n",
    "    * `registration`: user's time of registration\n",
    "    * `gender`: gender of the user\n",
    "    * `artist`: artist of the song the user is playing at the event\n",
    "    * `song`: song title the user is playing at the event\n",
    "    * `length`: length of the session\n",
    " \n",
    " \n",
    " * the data will be downloaded from Github and contained in an [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- [Predicting Customer Churn with Amazon Machine Learning](https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/)\n",
    "-[Customer Churn Prediction with XGBoost](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sagemaker as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment to use SageMaker and to organize the project data and artifacts in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sm.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sm.get_execution_role()\n",
    "smclient = sagemaker_session.sagemaker_client\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "prefix = \"music-streaming\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "\n",
    "### Ingest Data\n",
    "\n",
    "We ingest the simulated data from the public SageMaker S3 training database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_url = \"s3://sagemaker-sample-files/datasets/tabular/customer-churn/customer-churn-data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.s3.S3Downloader().download(data_source_url, \"./data\")\n",
    "shutil.unpack_archive(\"data/customer-churn-data.zip\", \"data/\", format=\"zip\")\n",
    "\n",
    "[\n",
    "    shutil.unpack_archive(k, k.parent / \"raw\", format=\"zip\")\n",
    "    for k in Path(\"data\").glob(\"simu*\")\n",
    "]\n",
    "shutil.unpack_archive(\"data/sample.zip\", \"data/raw\", format=\"zip\")\n",
    "[k.unlink() for k in Path(\"data\").glob(\"*.zip\")];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the raw data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.s3.S3Uploader.upload(\"data/raw/\", f\"s3://{bucket}/{prefix}/data/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing `json` to `csv` \n",
    "* Here we used a Processing Job to convert the raw streaming data files downloaded from the github repo (`simu-*.zip` files) to a full, CSV formatted file for Data Wrangler Ingestion purpose.\n",
    "you are importing the raw streaming data files downloaded from the github repo (`simu-*.zip` files). The raw JSON files were converted to CSV format and combined to one file for Data Wrangler Ingestion purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the script used for the Processing job, and we save it into a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing_predw.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "start_time = time.time()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--processing-output-filename\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    input_jsons = glob.glob(\"/opt/ml/processing/input/data/**/*.json\", recursive=True)\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "    for name in input_jsons:\n",
    "        print(\"\\nStarting file: {}\".format(name))\n",
    "        df = pd.read_json(name, lines=True)\n",
    "        df_all = df_all.append(df)\n",
    "\n",
    "    output_filename = args.processing_output_filename\n",
    "    final_features_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/output\", output_filename\n",
    "    )\n",
    "    print(\"Saving processed data to {}\".format(final_features_output_path))\n",
    "    df_all.to_csv(final_features_output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the files to process and add them to the `ProcessingInput` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_uris = [\n",
    "    f\"s3://{k}\" for k in s3.ls(f\"s3://{bucket}/{prefix}/data/json/\") if \"simu\" in k\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_inputs = []\n",
    "for uri in s3_input_uris:\n",
    "    name = uri.split(\"/\")[-1].split(\".\")[0]\n",
    "    processing_input = ProcessingInput(\n",
    "        source=uri, input_name=name, destination=f\"/opt/ml/processing/input/data/{name}\"\n",
    "    )\n",
    "    processing_inputs.append(processing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_output_path = f\"s3://{bucket}/{prefix}/data/processing\"\n",
    "final_features_filename = \"full_data.csv\"\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing_predw.py\",\n",
    "    inputs=processing_inputs,\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=processing_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\"--processing-output-filename\", final_features_filename],\n",
    "    wait=False,\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration on Sample Data\n",
    "\n",
    "While the Processing Job is underway, we can look into the data.  \n",
    "Due to the size of the data (~2GB), you will start exploring our data starting with a smaller sample, decide which pre-processing steps are necessary, and apply them to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_name = \"./data/raw/sample.json\"\n",
    "\n",
    "sample = pd.read_json(sample_file_name, lines=True)\n",
    "with sample.option_context(\"display.max_columns\", 100):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Let's take a look at our categorical columns first: `page`, `auth`, `level`, `location`, `userAgent`, `gender`, `artist`, and `song`, and start with looking at unique values for `page`, `auth`, `level`, and `gender` since the other three have many unique values and you will take a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"page\", \"auth\", \"level\", \"gender\"]\n",
    "cat_columns_long = [\"location\", \"userAgent\", \"artist\", \"song\", \"userId\"]\n",
    "for col in cat_columns:\n",
    "    print(\"The unique values in column {} are: {}\".format(col, sample[col].unique()))\n",
    "for col in cat_columns_long:\n",
    "    print(\"There are {} unique values  in column {}\".format(sample[col].nunique(), col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key observations from the above information\n",
    "\n",
    "* There are 101 unique users with 72 unique locations, this information may not be useful as a categorical feature. You can parse this field and only keep State information, but even that will give us 50 unique values in this category, so you can either remove this column or bucket it to a higher level (NY --> Northeast).\n",
    "* Artist and song details might not be helpful as categorical features as there are too many categories; you can quantify these to a user level, i.e. how many artists this user has listened to in total, how many songs this user has played in the last week, last month, in 180 days, in 365 days. You can also bring in external data to get song genres and other artist attributes to enrich this feature.\n",
    "* In the column `page`,  'Thumbs Down', 'Thumbs Up', 'Add to Playlist', 'Roll Advert','Help', 'Add Friend', 'Downgrade', 'Upgrade', and 'Error' can all be great features to churn analysis. You will aggregate them to user-level later. There is a \"cancellation confirmation\" value that can be used for the churn indicator.\n",
    "\n",
    "* Let's take a look at the column `userAgent`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserAgent contains little useful information, but if you care about the browser type and mac/windows difference, you can parse the text and extract the information. Sometimes businesses would love to analyze user behavior based on their App version and device type (iOS v.s. Android), so these could be useful information. In this use case, for modeling purpose, we will remove this column. but you can keep it as a filter for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"location\", \"userAgent\"]\n",
    "sample = sample.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the timestamp columns `ts` and `registration`. We can convert the event timestamp `ts` to year, month, week, day, day of the week, and hour of the day. The registration time should be the same for the same user, so we can aggregate this value to user-level and create a time delta column to calculate the time between registration and the newest event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"date\"] = pd.to_datetime(sample[\"ts\"], unit=\"ms\")\n",
    "sample[\"ts_year\"] = sample[\"date\"].dt.year\n",
    "sample[\"ts_month\"] = sample[\"date\"].dt.month\n",
    "sample[\"ts_week\"] = sample[\"date\"].dt.week\n",
    "sample[\"ts_day\"] = sample[\"date\"].dt.day\n",
    "sample[\"ts_dow\"] = sample[\"date\"].dt.weekday\n",
    "sample[\"ts_hour\"] = sample[\"date\"].dt.hour\n",
    "sample[\"ts_date_day\"] = sample[\"date\"].dt.date\n",
    "sample[\"ts_is_weekday\"] = [1 if x in [0, 1, 2, 3, 4] else 0 for x in sample[\"ts_dow\"]]\n",
    "sample[\"registration_ts\"] = pd.to_datetime(sample[\"registration\"], unit=\"ms\").dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Churn\n",
    "\n",
    "In this use case, you will use `page == \"Cancellation Confirmation\"` as the indicator of a user churn. You can also use `page == 'downgrade` if you are interested in users downgrading their payment plan. There are ~13% users churned, so you will need to up-sample or down-sample the full dataset to deal with the imbalanced class, or carefully choose your algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"There are {:.2f}% of users churned in this dataset\".format(\n",
    "        (\n",
    "            (sample[sample[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].nunique())\n",
    "            / sample[\"userId\"].nunique()\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can label a user by adding a churn label at a event level then aggregate this value to user level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"churned_event\"] = [\n",
    "    1 if x == \"Cancellation Confirmation\" else 0 for x in sample[\"page\"]\n",
    "]\n",
    "sample[\"user_churned\"] = sample.groupby(\"userId\")[\"churned_event\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced Class\n",
    "\n",
    "Imbalanced class (much more positive cases than negative cases) is very common in churn analysis. It can be misleading for some machine learning model as the accuracy will be biased towards the majority class. Some useful tactics to deal with imbalanced class are [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html), use algorithms that are less sensitive to imbalanced class like a tree-based algorithm or use a cost-sensitive algorithm that penalizes wrongly classified minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Summarize every pre-processing steps you have covered:\n",
    "* null removals\n",
    "* drop irrelevant columns\n",
    "* convert event timestamps to features used for analysis and modeling: year, month, week, day, day of week, hour, date, if the day is weekday or weekend, and convert registration timestamp to UTC.\n",
    "* create labels (whether the user churned eventually), which is calculated by if one churn event happened in the user's history, you can label the user as a churned user (1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Data\n",
    "\n",
    "Based on the available data, look at every column, and decide if you can create a feature from it. For all the columns, here are some directions to explore:\n",
    "\n",
    "    * `ts`: distribution of activity time: time of the day, day of the week\n",
    "    * `sessionId`: average number of sessions per user\n",
    "    * `page`:  number of thumbs up/thumbs down, added to the playlist, ads, add friend, if the user has downgrade or upgrade the plan, how many errors the user has encountered.\n",
    "    * `level`: if the user is a free or paid user\n",
    "    * `registration`: days the user being active, time the user joined the service\n",
    "    * `gender`: gender of the user\n",
    "    * `artist`: average number of artists the user listened to\n",
    "    * `song`: average number of songs listened per user\n",
    "    * `length`: average time spent per day per user\n",
    "   \n",
    "**Activity Time**\n",
    "\n",
    "1. Weekday v.s. weekend trends for churned users and active users. It seems like churned users are more active on weekdays than weekends whereas active users do not show a strong difference between weekday v.s. weekends. You can create some features from here: for each user, average events per day -- weekends, average events per day -- weekdays. You can also create features - average events per day of the week, but that will be converted to 7 features after one-hot-encoding, which may be less informative than the previous method.\n",
    "2. In terms of hours active during a day, our simulated data did not show a significant difference between day and night for both sets of users. You can have it on your checklist for your analysis, and similarly for the day of the month, the month of the year when you have more than a year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "events_per_day_per_user = (\n",
    "    sample.groupby([\"userId\", \"ts_date_day\", \"ts_is_weekday\", \"user_churned\"])\n",
    "    .agg({\"page\": \"count\"})\n",
    "    .reset_index()\n",
    ")\n",
    "events_dist = (\n",
    "    events_per_day_per_user.groupby([\"userId\", \"ts_is_weekday\", \"user_churned\"])\n",
    "    .agg({\"page\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "def trend_plot(\n",
    "    df,\n",
    "    plot_type,\n",
    "    x,\n",
    "    y,\n",
    "    hue=None,\n",
    "    title=None,\n",
    "    x_axis=None,\n",
    "    y_axis=None,\n",
    "    xticks=None,\n",
    "    yticks=None,\n",
    "):\n",
    "    if plot_type == \"box\":\n",
    "        fig = sns.boxplot(x=\"page\", y=y, data=df, hue=hue, orient=\"h\")\n",
    "    elif plot_type == \"bar\":\n",
    "        fig = sns.barplot(x=x, y=y, data=df, hue=hue)\n",
    "\n",
    "    sns.set(rc={\"figure.figsize\": (12, 3)})\n",
    "    sns.set_palette(\"Set2\")\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.yticks([0, 1], yticks)\n",
    "    return plt.show(fig)\n",
    "\n",
    "\n",
    "trend_plot(\n",
    "    events_dist,\n",
    "    \"box\",\n",
    "    \"page\",\n",
    "    \"user_churned\",\n",
    "    \"ts_is_weekday\",\n",
    "    \"Weekday V.S. Weekends - Average events per day per user\",\n",
    "    \"average events per user per day\",\n",
    "    yticks=[\"active users\", \"churned users\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_per_hour_per_user = (\n",
    "    sample.groupby([\"userId\", \"ts_date_day\", \"ts_hour\", \"user_churned\"])\n",
    "    .agg({\"page\": \"count\"})\n",
    "    .reset_index()\n",
    ")\n",
    "events_dist = (\n",
    "    events_per_hour_per_user.groupby([\"userId\", \"ts_hour\", \"user_churned\"])\n",
    "    .agg({\"page\": \"mean\"})\n",
    "    .reset_index()\n",
    "    .groupby([\"ts_hour\", \"user_churned\"])\n",
    "    .agg({\"page\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "trend_plot(\n",
    "    events_dist,\n",
    "    \"bar\",\n",
    "    \"ts_hour\",\n",
    "    \"page\",\n",
    "    \"user_churned\",\n",
    "    \"Hourly activity - Average events per hour of day per user\",\n",
    "    \"hour of the day\",\n",
    "    \"average events per user per hour\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listening Behavior**\n",
    "\n",
    "You can look at some basic stats for a user's listening habits. Churned users generally listen to a wider variety of songs and artists and spend more time on the App/be with the App longer.\n",
    "* Average total: number of sessions, App usage length, number of songs listened, number of artists listened per user, number of ad days active\n",
    "* Average daily: number of sessions, App usage length, number of songs listened, number of artists listened per user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_per_user = (\n",
    "    sample.groupby([\"userId\", \"user_churned\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"sessionId\": \"count\",\n",
    "            \"song\": \"nunique\",\n",
    "            \"artist\": \"nunique\",\n",
    "            \"length\": \"sum\",\n",
    "            \"ts_date_day\": \"count\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "avg_stats_group = (\n",
    "    stats_per_user.groupby([\"user_churned\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"sessionId\": \"mean\",\n",
    "            \"song\": \"mean\",\n",
    "            \"artist\": \"mean\",\n",
    "            \"length\": \"mean\",\n",
    "            \"ts_date_day\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Average total: number of sessions, App usage length, number of songs listened, number of artists listened per user, days active: \"\n",
    ")\n",
    "avg_stats_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_per_user = (\n",
    "    sample.groupby([\"userId\", \"ts_date_day\", \"user_churned\"])\n",
    "    .agg(\n",
    "        {\"sessionId\": \"count\", \"song\": \"nunique\", \"artist\": \"nunique\", \"length\": \"sum\"}\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "avg_stats_group = (\n",
    "    stats_per_user.groupby([\"user_churned\"])\n",
    "    .agg({\"sessionId\": \"mean\", \"song\": \"mean\", \"artist\": \"mean\", \"length\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "print(\n",
    "    \"Average daily: number of sessions, App usage length, number of songs listened, number of artists listened per user: \"\n",
    ")\n",
    "avg_stats_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**App Usage Behavior**\n",
    "\n",
    "You can further explore how the users are using the App besides just listening: number of thumbs up/thumbs down, added to playlist, ads, add friend, if the user has downgrade or upgrade the plan, how many errors the user has encountered. Churned users are slightly more active than other users, and also encounter more errors, listened to more ads, and more downgrade and upgrade. These can be numerical features (number of total events per type per user), or more advanced time series numerical features (errors in last 7 days, errors in last month, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list = [\n",
    "    \"NextSong\",\n",
    "    \"Thumbs Down\",\n",
    "    \"Thumbs Up\",\n",
    "    \"Add to Playlist\",\n",
    "    \"Roll Advert\",\n",
    "    \"Add Friend\",\n",
    "    \"Downgrade\",\n",
    "    \"Upgrade\",\n",
    "    \"Error\",\n",
    "]\n",
    "usage_column_name = []\n",
    "for event in events_list:\n",
    "    event_name = \"_\".join(event.split()).lower()\n",
    "    usage_column_name.append(event_name)\n",
    "    sample[event_name] = [1 if x == event else 0 for x in sample[\"page\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_use_per_user = (\n",
    "    sample.groupby([\"userId\", \"user_churned\"])[usage_column_name].sum().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_use_group = (\n",
    "    app_use_per_user.groupby([\"user_churned\"])[usage_column_name].mean().reset_index()\n",
    ")\n",
    "app_use_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataWrangler for Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good understanding of the data and decided which steps are needed to pre-process the data, we can use Data Wrangler to design and run the process, without writing all the code for the SageMaker Processing Job.\n",
    "\n",
    "We can create a new `.flow` file, or use the template included in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_output_filename = f\"{processing_output_path}/{final_features_filename}\"\n",
    "flow_file = \"dw_example.template\"\n",
    "\n",
    "# read flow file and change the s3 location to our `processing_output_filename`\n",
    "with open(flow_file, \"r\") as f:\n",
    "    flow = json.loads(f.read())\n",
    "    flow[\"nodes\"][0][\"parameters\"][\"dataset_definition\"][\"s3ExecutionContext\"][\n",
    "        \"s3Uri\"\n",
    "    ] = processing_output_filename\n",
    "\n",
    "with open(\"dw_example.flow\", \"w\") as f:\n",
    "    json.dump(flow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering - Local Version\n",
    "Here we indicate a possible feature engineering. These steps mirror the DataWrangler template flow file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(sample_file_name, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant columns\n",
    "\n",
    "From the first look of data, you can notice that columns `lastName`, `firstName`, `method` and `status` are not relevant features. These will be dropped from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"method\", \"status\", \"lastName\", \"firstName\"]\n",
    "df = df.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for null values\n",
    "\n",
    "You are going to remove all events without an `userId` assigned since you are predicting which recognized user will churn from our service. In this case, all the rows(events) have a `userId` and `sessionId` assigned, but you will still run this step for the full dataset. For other columns, there are ~3% of data that are missing some demographic information of the users, and ~20% missing the song attributes, which is because the events contain not only playing a song, but also other actions including login and log out, downgrade, cancellation, etc. There are ~3% of users that do not have a registration time, so you will remove these anonymous users from the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percentage of the value missing in each column is: \")\n",
    "df.isnull().sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"userId\"].isnull()]\n",
    "df = df[~df[\"registration\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ts\"] = pd.to_datetime(df.ts, unit=\"ms\")\n",
    "df[\"registration\"] = pd.to_datetime(df[\"registration\"], unit=\"ms\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df[\"ts\"].dt.date\n",
    "df[\"ts_dow\"] = df[\"ts\"].dt.weekday\n",
    "df[\"ts_is_weekday\"] = (df[\"ts_dow\"].isin([0, 1, 2, 3, 4])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"churned_event\"] = (df[\"page\"] == \"Cancellation Confirmation\").astype(int)\n",
    "df[\"user_churned\"] = df.groupby(\"userId\")[\"churned_event\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list = [\n",
    "    \"NextSong\",\n",
    "    \"Thumbs Down\",\n",
    "    \"Thumbs Up\",\n",
    "    \"Add to Playlist\",\n",
    "    \"Roll Advert\",\n",
    "    \"Add Friend\",\n",
    "    \"Downgrade\",\n",
    "    \"Upgrade\",\n",
    "    \"Error\",\n",
    "]\n",
    "df[\"events\"] = (\n",
    "    df.page.str.lower().str.replace(\" \", \"_\").where(df.page.isin(events_list))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"events\"], prefix=\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = (\n",
    "    df.groupby([\"userId\", \"date\", \"ts_is_weekday\"])\n",
    "    .agg({\"page\": \"count\"})\n",
    "    .groupby([\"userId\", \"ts_is_weekday\"])[\"page\"]\n",
    "    .mean()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"average_events_weekend\", 1: \"average_events_weekday\"})\n",
    ")\n",
    "\n",
    "\n",
    "base_df_daily = (\n",
    "    df.groupby([\"userId\", \"date\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"page\": \"count\",\n",
    "            \"events_nextsong\": \"sum\",\n",
    "            \"events_roll_advert\": \"sum\",\n",
    "            \"events_error\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "feature34 = (\n",
    "    base_df_daily.groupby([\"userId\", \"date\"])\n",
    "    .tail(7)\n",
    "    .groupby([\"userId\"])\n",
    "    .agg({\"events_nextsong\": \"sum\", \"events_roll_advert\": \"sum\", \"events_error\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"events_nextsong\": \"num_songs_played_7d\",\n",
    "            \"events_roll_advert\": \"num_ads_7d\",\n",
    "            \"events_error\": \"num_error_7d\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "feature5 = (\n",
    "    base_df_daily.groupby([\"userId\", \"date\"])\n",
    "    .tail(30)\n",
    "    .groupby([\"userId\"])\n",
    "    .agg({\"events_nextsong\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename(columns={\"events_nextsong\": \"num_songs_played_30d\"})\n",
    ")\n",
    "feature6 = (\n",
    "    base_df_daily.groupby([\"userId\", \"date\"])\n",
    "    .tail(90)\n",
    "    .groupby([\"userId\"])\n",
    "    .agg({\"events_nextsong\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename(columns={\"events_nextsong\": \"num_songs_played_90d\"})\n",
    ")\n",
    "# num_artists, num_songs, num_ads, num_thumbsup, num_thumbsdown, num_playlist, num_addfriend, num_error, user_downgrade,\n",
    "# user_upgrade, percentage_ad, days_since_active\n",
    "base_df_user = (\n",
    "    df.groupby([\"userId\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"page\": \"count\",\n",
    "            \"events_nextsong\": \"sum\",\n",
    "            \"artist\": \"nunique\",\n",
    "            \"song\": \"nunique\",\n",
    "            \"events_thumbs_down\": \"sum\",\n",
    "            \"events_thumbs_up\": \"sum\",\n",
    "            \"events_add_to_playlist\": \"sum\",\n",
    "            \"events_roll_advert\": \"sum\",\n",
    "            \"events_add_friend\": \"sum\",\n",
    "            \"events_downgrade\": \"max\",\n",
    "            \"events_upgrade\": \"max\",\n",
    "            \"events_error\": \"sum\",\n",
    "            \"date\": \"max\",\n",
    "            \"registration\": \"min\",\n",
    "            \"user_churned\": \"max\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "base_df_user[\"percentage_ad\"] = (\n",
    "    base_df_user[\"events_roll_advert\"] / base_df_user[\"page\"]\n",
    ")\n",
    "base_df_user[\"days_since_active\"] = (\n",
    "    base_df_user[\"date\"] - base_df_user[\"registration\"]\n",
    ").dt.days\n",
    "# repeats ratio\n",
    "base_df_user[\"repeats_ratio\"] = (\n",
    "    1 - base_df_user[\"song\"] / base_df_user[\"events_nextsong\"]\n",
    ")\n",
    "\n",
    "# num_sessions, avg_time_per_session, avg_events_per_session,\n",
    "base_df_session = (\n",
    "    df.groupby([\"userId\", \"sessionId\"])\n",
    "    .agg({\"length\": \"sum\", \"page\": \"count\", \"date\": \"min\"})\n",
    "    .reset_index()\n",
    ")\n",
    "base_df_session[\"prev_session_ts\"] = base_df_session.groupby([\"userId\"])[\"date\"].shift(\n",
    "    1\n",
    ")\n",
    "base_df_session[\"gap_session\"] = (\n",
    "    base_df_session[\"date\"] - base_df_session[\"prev_session_ts\"]\n",
    ").dt.days\n",
    "user_sessions = (\n",
    "    base_df_session.groupby(\"userId\")\n",
    "    .agg(\n",
    "        {\"sessionId\": \"count\", \"length\": \"mean\", \"page\": \"mean\", \"gap_session\": \"mean\"}\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"sessionId\": \"num_sessions\",\n",
    "            \"length\": \"avg_time_per_session\",\n",
    "            \"page\": \"avg_events_per_session\",\n",
    "            \"gap_session\": \"avg_gap_between_session\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# merge features together\n",
    "base_df[\"userId\"] = base_df[\"userId\"]  # .astype(\"int\")\n",
    "final_feature_df = base_df.merge(feature34, how=\"left\", on=\"userId\")\n",
    "final_feature_df = final_feature_df.merge(feature5, how=\"left\", on=\"userId\")\n",
    "final_feature_df = final_feature_df.merge(feature6, how=\"left\", on=\"userId\")\n",
    "final_feature_df = final_feature_df.merge(user_sessions, how=\"left\", on=\"userId\")\n",
    "final_feature_df = final_feature_df.merge(base_df_user, how=\"left\", on=\"userId\")\n",
    "\n",
    "final_feature_df = final_feature_df.fillna(0)\n",
    "# renaming columns\n",
    "final_feature_df.columns = [\n",
    "    \"userId\",\n",
    "    \"average_events_weekend\",\n",
    "    \"average_events_weekday\",\n",
    "    \"num_songs_played_7d\",\n",
    "    \"num_ads_7d\",\n",
    "    \"num_error_7d\",\n",
    "    \"num_songs_played_30d\",\n",
    "    \"num_songs_played_90d\",\n",
    "    \"num_sessions\",\n",
    "    \"avg_time_per_session\",\n",
    "    \"avg_events_per_session\",\n",
    "    \"avg_gap_between_session\",\n",
    "    \"num_events\",\n",
    "    \"num_songs\",\n",
    "    \"num_artists\",\n",
    "    \"num_unique_songs\",\n",
    "    \"num_thumbs_down\",\n",
    "    \"num_thumbs_up\",\n",
    "    \"num_add_to_playlist\",\n",
    "    \"num_ads\",\n",
    "    \"num_add_friend\",\n",
    "    \"num_downgrade\",\n",
    "    \"num_upgrade\",\n",
    "    \"num_error\",\n",
    "    \"ts_date_day\",\n",
    "    \"registration\",\n",
    "    \"user_churned\",\n",
    "    \"percentage_ad\",\n",
    "    \"days_since_active\",\n",
    "    \"repeats_ratio\",\n",
    "]\n",
    "# only keep created feature columns\n",
    "final_feature_df = final_feature_df[\n",
    "    [\n",
    "        \"userId\",\n",
    "        \"user_churned\",\n",
    "        \"average_events_weekend\",\n",
    "        \"average_events_weekday\",\n",
    "        \"num_songs_played_7d\",\n",
    "        \"num_ads_7d\",\n",
    "        \"num_error_7d\",\n",
    "        \"num_songs_played_30d\",\n",
    "        \"num_songs_played_90d\",\n",
    "        \"num_sessions\",\n",
    "        \"avg_time_per_session\",\n",
    "        \"avg_events_per_session\",\n",
    "        \"avg_gap_between_session\",\n",
    "        \"num_events\",\n",
    "        \"num_songs\",\n",
    "        \"num_artists\",\n",
    "        \"num_thumbs_down\",\n",
    "        \"num_thumbs_up\",\n",
    "        \"num_add_to_playlist\",\n",
    "        \"num_ads\",\n",
    "        \"num_add_friend\",\n",
    "        \"num_downgrade\",\n",
    "        \"num_upgrade\",\n",
    "        \"num_error\",\n",
    "        \"percentage_ad\",\n",
    "        \"days_since_active\",\n",
    "        \"repeats_ratio\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
