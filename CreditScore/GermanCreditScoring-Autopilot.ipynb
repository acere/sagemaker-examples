{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score prediction with Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sagemaker as sm\n",
    "from sagemaker.automl import automl\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "<!-- The data is downloaded from Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"s3://sagemaker-sample-files/datasets/tabular/uci_statlog_german_credit_data/german_credit_data.csv\"\n",
    "target_name = \"risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_true_value = df[target_name].value_counts().index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split `train` and `test` datasets\n",
    "\n",
    "Reserve a fraction of the records for out of band testing, either as batch transform or as a inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.sample(frac=0.15, random_state=42)\n",
    "df_train = df.drop(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation: Autopilot\n",
    "\n",
    "Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets.   \n",
    "We will use the AutoML estimator from SageMaker Python SDK to invoke Autopilot to find the best ML pipeline to train a model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is developed to run in Amazon SageMaker Studio. We can get the sesssion and role and other parameters from the environment using the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = sm.Session()\n",
    "sm_client = sm_session.sagemaker_client\n",
    "s3 = s3fs.S3FileSystem()\n",
    "region = sm_session.boto_region_name\n",
    "role = sm.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data will be stored in the default bucket, using the folder name as prefix to organize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sm_session.default_bucket()\n",
    "prefix = f\"{Path().resolve().name}-{datetime.now():%Y%m%d%H%M}\"\n",
    "print(\n",
    "    f\"All files be be stored in \\033[93m{bucket}\\033[0m\\n\"\n",
    "    f\"with prefix \\033[93m{prefix}\\033[0m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start uploading the training dataset to S3.  \n",
    "Currently Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order by name, is expected to have a header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_uri = f\"s3://{bucket}/{prefix}/train.csv\"\n",
    "\n",
    "df_train.to_csv(train_dataset_uri, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the definition of the AUtopilot task, we specify the kind of problem, a `BinaryClassification`, and we limit the total number of models considered to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_obj = automl.AutoML(\n",
    "    role=role,\n",
    "    target_attribute_name=target_name,\n",
    "    output_path=f\"s3://{bucket}/{prefix}/automl-output\",\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    max_candidates=30,\n",
    "    job_objective={\"MetricName\": \"AUC\", \"MetricName\": \"F1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch the Autopilot job by calling the fit method of the AutoML estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_obj.fit(inputs=train_dataset_uri, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Autopilot job consists of the following high-level steps :\n",
    "\n",
    "- Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.\n",
    "- Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level.\n",
    "- Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_obj.current_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `describe_auto_ml_job method` to check the status of our SageMaker Autopilot job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    status = automl_obj.describe_auto_ml_job()[\"AutoMLJobStatus\"]\n",
    "    print(status)\n",
    "    if status != \"InProgress\":\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model candidates\n",
    "The Autopilot job is completed, and we now have a set of models with their associated performance metric.\n",
    "Let's consider the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list = automl_obj.list_candidates(\n",
    "    max_results=10, sort_by=\"FinalObjectiveMetricValue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.json_normalize(candidates_list)[\n",
    "    [\n",
    "        \"CandidateName\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.Value\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.MetricName\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"FinalAutoMLJobObjectiveMetric.Value\": \"metric_value\",\n",
    "        \"FinalAutoMLJobObjectiveMetric.MetricName\": \"metric_name\",\n",
    "        \"CandidateName\": \"candidate_name\",\n",
    "    }\n",
    ")\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Top Candidates\n",
    "We can start running inference on the top candidates. In SageMaker, you can perform inference in two ways: online endpoint inference or batch transform inference. Lets focus on batch transform inference.\n",
    "\n",
    "We'll perform batch transform on our top candidates and analyze some custom metrics from our top candidates' prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_candidates = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the  test dataset we set aside previously, `df_test`. We need to upload this data to S3.   \n",
    "For Batch prediction jobs, the input data must be without headers, and the order and number of features columns must match that of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_uri = f\"s3://{bucket}/{prefix}/test.csv\"\n",
    "df_test.drop(columns=target_name).to_csv(test_dataset_uri, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformers(\n",
    "    candidate: dict, automl_instance: automl.AutoML, s3_transform_output_path, **kwarg\n",
    "):\n",
    "    \"\"\"Create a transformer from a Automl model candidate\"\"\"\n",
    "    model = automl_instance.create_model(\n",
    "        name=candidate[\"CandidateName\"], candidate=candidate, **kwarg\n",
    "    )\n",
    "\n",
    "    output_path = f\"{s3_transform_output_path}{candidate['CandidateName']}/\"\n",
    "\n",
    "    return model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        assemble_with=\"Line\",\n",
    "        output_path=output_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problem types, the inference containers generated by SageMaker Autopilot allow you to select the response content for predictions. Valid inference response content are defined below for binary classification and multiclass classification problem types.\n",
    "\n",
    "- 'predicted_label' - predicted class\n",
    "- 'probability' - In binary classification, the probability that the result is predicted as the second or True class in the target column. In multiclass classification, the probability of the winning class.\n",
    "- 'labels' - list of all possible classes\n",
    "- 'probabilities' - list of all probabilities for all classes (order corresponds with 'labels')\n",
    "\n",
    "By default the inference contianers are configured to generate the 'predicted_label'.\n",
    "\n",
    "In this example we use ‘predicted_label’ and ‘probability’ to demonstrate how to evaluate the models with custom metrics. \n",
    "For the German Credit Score dataset, the second or True class is the value `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_response_keys = [\"predicted_label\", \"probability\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = f\"s3://{bucket}/{prefix}/inference-results/\"\n",
    "transformers_list = [\n",
    "    create_transformers(\n",
    "        c, automl_obj, batch_output, inference_response_keys=inference_response_keys\n",
    "    )\n",
    "    for c in candidates_list[:top_n_candidates]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(transformer, input_dataset_uri):\n",
    "    \"\"\"Start a Batch Transform job for a transformer given an input dataset\"\"\"\n",
    "    try:\n",
    "        transformer.transform(\n",
    "            data=input_dataset_uri,\n",
    "            data_type=\"S3Prefix\",\n",
    "            content_type=\"text/csv\",\n",
    "            split_type=\"Line\",\n",
    "            wait=False,\n",
    "        )\n",
    "        print(f\"Starting transform job {transformer._current_job_name}\")\n",
    "    except Exception as e:\n",
    "        # catch also exception due to account-level service limits\n",
    "        print(f\"{transformer._current_job_name} failed with error {e}\")\n",
    "        return\n",
    "    return transformer._current_job_name\n",
    "\n",
    "\n",
    "# We use this starting time to filter the list of transformation job when we monitoring the progress\n",
    "start_time = time.time()\n",
    "batch_predictions_names = [\n",
    "    batch_predict(t, test_dataset_uri) for t in transformers_list\n",
    "]\n",
    "\n",
    "# remove empty entries\n",
    "batch_predictions_names = [b for b in batch_predictions_names if b is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wait for our transform jobs to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    job_list = pd.DataFrame(\n",
    "        sm_client.list_transform_jobs(CreationTimeAfter=start_time)[\n",
    "            \"TransformJobSummaries\"\n",
    "        ]\n",
    "    )\n",
    "    num_transform_jobs = (\n",
    "        job_list[job_list[\"TransformJobName\"].isin(batch_predictions_names)][\n",
    "            \"TransformJobStatus\"\n",
    "        ]\n",
    "        == \"InProgress\"\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"{num_transform_jobs} out of {len(batch_predictions_names)} transform jobs are running.\"\n",
    "    )\n",
    "    if num_transform_jobs == 0:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Inference Results\n",
    "\n",
    "Now we analyze our inference results. The batch transform results are stored in S3, we load them into a dictionary, using the model name as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {\n",
    "    k.model_name: pd.read_csv(k.output_path + \"test.csv.out\", header=None)\n",
    "    for k in transformers_list\n",
    "    if s3.exists(k.output_path + \"test.csv.out\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an array of the ground truth labels for conveninence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test[target_name] == target_true_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate two common metrics for classificaitno problems, the *Area Under the Receiver Operating Characteristic Curve*, or `ROC AUC`, and the *Average Precision*, or `AP` from the prediction probabilities, and the test `F1` score from the predicted label.\n",
    "\n",
    "We also include the `F1` score pulled from the Autopilot candidate description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_metrics = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        candidate: {\n",
    "            \"AUC\": roc_auc_score(labels, prediction[1]),\n",
    "            \"AP\": average_precision_score(labels, prediction[1]),\n",
    "            \"F1_test\": f1_score(labels, prediction[0] == target_true_value),\n",
    "        }\n",
    "        for candidate, prediction in predictions_dict.items()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "models_metrics = models_metrics.join(\n",
    "    models.set_index(\"candidate_name\").metric_value.rename(\"F1_autopilot\")\n",
    ")\n",
    "models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_dict = {\n",
    "    candidate: roc_curve(labels, prediction[1])\n",
    "    for candidate, prediction in predictions_dict.items()\n",
    "}\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor=\"w\", edgecolor=\"k\")\n",
    "[plt.plot(i[0], i[1], label=k) for k, i in roc_curve_dict.items()]\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_dict = {\n",
    "    candidate: precision_recall_curve(labels, prediction[1])\n",
    "    for candidate, prediction in predictions_dict.items()\n",
    "}\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor=\"w\", edgecolor=\"k\")\n",
    "[plt.plot(i[1], i[0], label=k) for k, i in precision_recall_dict.items()]\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias - Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_dataset_uri,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=target_name,\n",
    "    headers=df_train.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = candidates_list[0][\"CandidateName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selected = automl_obj.create_model(\n",
    "    name=selected_model_name,\n",
    "    candidate=candidates_list[0],\n",
    "    inference_response_keys=inference_response_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = clarify.ModelConfig(\n",
    "    model_name=selected_model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_config = clarify.ModelPredictedLabelConfig(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[target_true_value],\n",
    "    facet_name=\"age\",\n",
    "    facet_values_or_threshold=[35],\n",
    "    group_name=\"status_sex\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\" The bias reports in html, jupyter notebook, and PDF formats is at {bias_report_output_path}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=[df_train.drop(columns=target_name).astype(str).iloc[0].values.tolist()],\n",
    "    num_samples=15,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    ")\n",
    "\n",
    "explainability_output_path = f\"s3://{bucket}/{prefix}/clarify-explainability\"\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_dataset_uri,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    label=target_name,\n",
    "    headers=df_train.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    "    model_scores=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_explanations_out = pd.read_csv(\n",
    "    explainability_output_path + \"/explanations_shap/out.csv\"\n",
    ")\n",
    "feature_names = [\n",
    "    str.replace(c, \"_label0\", \"\") for c in local_explanations_out.columns.to_series()\n",
    "]\n",
    "local_explanations_out.columns = feature_names\n",
    "\n",
    "selected_example = 111\n",
    "print(\n",
    "    \"Example number:\",\n",
    "    selected_example,\n",
    "    \"\\nwith model prediction:\",\n",
    "    sum(local_explanations_out.iloc[selected_example]) > 0,\n",
    ")\n",
    "print(\"\\nFeature values -- Label\", df_train.iloc[selected_example])\n",
    "local_explanations_out.iloc[selected_example].plot(\n",
    "    kind=\"bar\",\n",
    "    title=\"Local explanation for the example number \" + str(selected_example),\n",
    "    rot=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sm_client.delete_model(ModelName=k['ModelName']) for k in sm_client.list_models()['Models']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k['ModelName'] for k in sm_client.list_models()['Models']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all files and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.rm(f\"s3://{bucket}/{prefix}\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !black-nb ."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "SageMaker",
   "language": "python",
   "name": "sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
