{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Face Detection and Recognition Model - Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "**Face detection**  \n",
    "In order to input only face pixels into the network, all input images are passed through a pretrained face detection and alignment model, [MTCNN detector](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html). The output of this model are landmark points and a bounding box corresponding to the face in the image. Using this output, the image is processed using affine transforms to generate the aligned face images which are input to the network.\n",
    "\n",
    "**Face feature generation**  \n",
    "For each face image, the model produces a fixed length embedding vector corresponding to the face in the image. The vectors from face images of a single person have a higher similarity than that from different persons. Therefore, the model is primarily used for face recognition/verification. It can also be used in other applications like facial feature based clustering.  \n",
    "\n",
    "**Model artifacts**     \n",
    "In this implementation, we use LResNet100E-IR, ResNet100 backend with [ArcFace](https://arxiv.org/abs/1801.07698) loss.   \n",
    "For both the MTCNN detector and the ResNet we use and use the pre-trained models from [ONNX Model Zoo](https://github.com/onnx/models), then import the [ONNX](http://onnx.ai/) files into MXNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and environment setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start installing the necessary libraries as indicated in the `requirement.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -q -r model/code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the libraries that we need to run this notebook. We also include a custom python library `mtcnn_detector.py`, that provides some useful abstractions for detecting and processing faces from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet.contrib.onnx.onnx2mx.import_model import import_model\n",
    "from skimage import transform as trans\n",
    "\n",
    "from model.code.mtcnn_detector import MtcnnDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the context to make sure to use a GPU if available (and properly configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine and set context\n",
    "if len(mx.test_utils.list_gpus()) == 0:\n",
    "    ctx = mx.cpu()\n",
    "else:\n",
    "    ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some file paths for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = Path(\"model\")\n",
    "code_local_path = model_local_path / \"code\"\n",
    "images_local_path = Path(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Face detection\n",
    "We start by downloading the pre-trained MTCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_mtcnn_model(i, dirname: str):\n",
    "    base_url = f\"https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{i+1}\"\n",
    "    mx.test_utils.download(url=f\"{base_url}-0001.params\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}-symbol.json\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}.caffemodel\", dirname=dirname)\n",
    "    mx.test_utils.download(url=f\"{base_url}.prototxt\", dirname=dirname)\n",
    "    return \"Done\"\n",
    "\n",
    "\n",
    "mtcnn_local_path = model_local_path / \"mtcnn-model\"\n",
    "\n",
    "[download_mtcnn_model(i, dirname=mtcnn_local_path) for i in range(4)]\n",
    "\n",
    "print(f\"MTCNN artifacts downloaded to `{mtcnn_local_path}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now configure and initialize the face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_threshold = [0.6, 0.7, 0.8]\n",
    "detector = MtcnnDetector(\n",
    "    model_folder=mtcnn_local_path.as_posix(),\n",
    "    ctx=ctx,\n",
    "    num_worker=1,\n",
    "    accurate_landmark=True,\n",
    "    threshold=det_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, bbox=None, landmark=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Preprocess images to detect and extract faces.\n",
    "\n",
    "    Returns a 3 x 112 x 112 (channels x width x height) numpy array.\n",
    "    \"\"\"\n",
    "    M = None\n",
    "    image_size = []\n",
    "    str_image_size = kwargs.get(\"image_size\", \"\")\n",
    "    # Assert input shape\n",
    "    if len(str_image_size) > 0:\n",
    "        image_size = [int(x) for x in str_image_size.split(\",\")]\n",
    "        if len(image_size) == 1:\n",
    "            image_size = [image_size[0], image_size[0]]\n",
    "        assert len(image_size) == 2\n",
    "        assert image_size[0] == 112\n",
    "        assert image_size[0] == 112 or image_size[1] == 96\n",
    "\n",
    "    # Do alignment using landmark points\n",
    "    if landmark is not None:\n",
    "        assert len(image_size) == 2\n",
    "        src = np.array(\n",
    "            [\n",
    "                [30.2946, 51.6963],\n",
    "                [65.5318, 51.5014],\n",
    "                [48.0252, 71.7366],\n",
    "                [33.5493, 92.3655],\n",
    "                [62.7299, 92.2041],\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        if image_size[1] == 112:\n",
    "            src[:, 0] += 8.0\n",
    "        dst = landmark.astype(np.float32)\n",
    "        tform = trans.SimilarityTransform()\n",
    "        tform.estimate(dst, src)\n",
    "        M = tform.params[0:2, :]\n",
    "        assert len(image_size) == 2\n",
    "        warped = cv2.warpAffine(img, M, (image_size[1], image_size[0]), borderValue=0.0)\n",
    "        return warped\n",
    "\n",
    "    # If no landmark points available, do alignment using bounding box. If no bounding box available use center crop\n",
    "    if M is None:\n",
    "        if bbox is None:\n",
    "            det = np.zeros(4, dtype=np.int32)\n",
    "            det[0] = int(img.shape[1] * 0.0625)\n",
    "            det[1] = int(img.shape[0] * 0.0625)\n",
    "            det[2] = img.shape[1] - det[0]\n",
    "            det[3] = img.shape[0] - det[1]\n",
    "        else:\n",
    "            det = bbox\n",
    "        margin = kwargs.get(\"margin\", 44)\n",
    "        bb = np.zeros(4, dtype=np.int32)\n",
    "        bb[0] = np.maximum(det[0] - margin / 2, 0)\n",
    "        bb[1] = np.maximum(det[1] - margin / 2, 0)\n",
    "        bb[2] = np.minimum(det[2] + margin / 2, img.shape[1])\n",
    "        bb[3] = np.minimum(det[3] + margin / 2, img.shape[0])\n",
    "        ret = img[bb[1] : bb[3], bb[0] : bb[2], :]\n",
    "        if len(image_size) > 0:\n",
    "            ret = cv2.resize(ret, (image_size[1], image_size[0]))\n",
    "        return ret\n",
    "\n",
    "\n",
    "def get_input(detector, face_img):\n",
    "    \"\"\"\n",
    "    Pass input images through face detector\n",
    "    \"\"\"\n",
    "    ret = detector.detect_face(face_img, det_type=0)\n",
    "    if ret is None:\n",
    "        return None\n",
    "    bbox, points = ret\n",
    "    if bbox.shape[0] == 0:\n",
    "        return None\n",
    "    bbox = bbox[0, 0:4]\n",
    "    points = points[0, :].reshape((2, 5)).T\n",
    "    # Call preprocess() to generate aligned images\n",
    "    nimg = preprocess(face_img, bbox, points, image_size=\"112,112\")\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)\n",
    "    aligned = np.transpose(nimg, (2, 0, 1))\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Face Detection\n",
    "\n",
    "We can now test visually that the face detector and the preprocessing function effectively detect and apply the correct transofrmations to the test images.\n",
    "\n",
    "Let's download the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download first image\n",
    "image1_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/player1.jpg\",\n",
    "    dirname=images_local_path,\n",
    ")\n",
    "# Download second image\n",
    "image2_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/player2.jpg\",\n",
    "    dirname=images_local_path,\n",
    ")\n",
    "\n",
    "img1 = cv2.imread(image1_path)\n",
    "img2 = cv2.imread(image2_path)\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Image1\")\n",
    "ax[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "ax[1].set_title(\"Image2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the detection + preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_preprocessed = get_input(detector, img1)\n",
    "img2_preprocessed = get_input(detector, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(np.transpose(img1_preprocessed, (1, 2, 0)))\n",
    "ax[0].set_title(\"Image1_preprocessed\")\n",
    "ax[1].imshow(np.transpose(img2_preprocessed, (1, 2, 0)))\n",
    "ax[1].set_title(\"Image2_preprocessed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Generate Feature Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LResNet100E-IR, we proceed in a similar way as previous section:\n",
    "\n",
    "1. Download pre-trained model\n",
    "2. Convert weights from ONNX to MXNet\n",
    "3. Initialize model\n",
    "4. Test\n",
    "\n",
    "We download the pre-trained ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = mx.test_utils.download(\n",
    "    \"https://s3.amazonaws.com/onnx-model-zoo/arcface/resnet100.onnx\",\n",
    "    dirname=model_local_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ctx, model):\n",
    "    \"\"\"\n",
    "    Import ONNX artifact and initializes the model\n",
    "    \"\"\"\n",
    "    image_size = (112, 112)\n",
    "    sym, arg_params, aux_params = import_model(model)\n",
    "\n",
    "    # Define and binds parameters to the network\n",
    "    model = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\n",
    "    model.bind(data_shapes=[(\"data\", (1, 3, image_size[0], image_size[1]))])\n",
    "    model.set_params(arg_params, aux_params)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_feature(model, aligned):\n",
    "    \"\"\"\n",
    "    Create feature vector from input face.\n",
    "\n",
    "    Macth input dimensions to input expected by the model.\n",
    "    Only process one image at the time.\n",
    "    \"\"\"\n",
    "    input_blob = np.expand_dims(aligned, axis=0)\n",
    "    data = mx.nd.array(input_blob)\n",
    "    db = mx.io.DataBatch(data=(data,))\n",
    "    model.forward(db, is_train=False)\n",
    "    embedding = model.get_outputs()[0].asnumpy()\n",
    "    embedding /= (embedding ** 2).sum() ** 0.5\n",
    "    return embedding.flatten()\n",
    "\n",
    "\n",
    "model = get_model(ctx, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n",
    "We can finally test the model. We will generate features for both test images and then compute two distance metrics between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = get_feature(model, img1_preprocessed)\n",
    "out2 = get_feature(model, img2_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of each inference is a 512-element vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.shape, out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute squared distance between embeddings\n",
    "dist = np.sum(np.square(out1 - out2))\n",
    "# Compute cosine similarity between embedddings\n",
    "sim = np.dot(out1, out2.T)\n",
    "# Print predictions\n",
    "print(\"Distance = %f\" % (dist))\n",
    "print(\"Similarity = %f\" % (sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the average processing time. This is not a rigoruous benchmark, but it gives us an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "get_feature(model, img1_preprocessed)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "mxnet18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
